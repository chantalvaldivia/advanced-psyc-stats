{
  "hash": "de4b520a67f4afd2135efb97a39d4e50",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayes_Lab_2\"\nformat: html\neditor: visual\n---\n\n\n\nFor Lab 1, you had explored the data and looked at models built via lm() and via brms(using default priors). You had also drawn posterior samples after fitting the model.\n\nFor Lab 2, we continue with the Palmer Penguins. And we will look more at distributions and priors.\n\nAgain, there will be conceptual questions to answer as you work through this example, and exercises.\n\n# Part 3: Distributions all the way down\n\nGiven it's a continuation of Lab 1, let's begin by loading relevant packages, cleaning/pre-processing the data, and fitting lm() and the default brm models\n\n## Setup: Packages and data\n\nWe load the primary packages.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(ggdist)\n```\n:::\n\n\n\nWe want the same data set up as in the last lab.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the penguins data\ndata(penguins, package = \"palmerpenguins\")\n\n# subset the data\nchinstrap <- penguins %>% \n  filter(species == \"Chinstrap\")\n\nglimpse(chinstrap)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 68\nColumns: 8\n$ species           <fct> Chinstrap, Chinstrap, Chinstrap, Chinstrap, Chinstra…\n$ island            <fct> Dream, Dream, Dream, Dream, Dream, Dream, Dream, Dre…\n$ bill_length_mm    <dbl> 46.5, 50.0, 51.3, 45.4, 52.7, 45.2, 46.1, 51.3, 46.0…\n$ bill_depth_mm     <dbl> 17.9, 19.5, 19.2, 18.7, 19.8, 17.8, 18.2, 18.2, 18.9…\n$ flipper_length_mm <int> 192, 196, 193, 188, 197, 198, 178, 197, 195, 198, 19…\n$ body_mass_g       <int> 3500, 3900, 3650, 3525, 3725, 3950, 3250, 3750, 4150…\n$ sex               <fct> female, male, male, female, male, female, female, ma…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n```\n\n\n:::\n:::\n\n\n\n## Models\n\nOnce again, we'll fit the model\n\n$$\n\\begin{align}\n\\text{bill_length_mm}_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i + \\epsilon_i \\\\\n\\epsilon_i & \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) ,\n\\end{align}\n$$\n\nwith both `lm()` and `brm()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# OLS\nfit1.ols <- lm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n\n# Bayes\nfit1.b <- brm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\n## Bayesians have many kinds of distributions\n\nIn Bayesian statistics, we have at least 6 distributions to keep track of. Those are:\n\n-   the likelihood distributions\n-   the prior parameter distribution (aka priors)\n-   the prior predictive distributions\n-   the posterior parameter distributions (aka posteriors)\n-   the posterior-predictive distribution\n\nIn many respect, it's distributions 'all the way down,' with Bayesians. This can be indeed be difficult to keep track of at first. But since this is true for any class of Bayesian models (not just regression), you'll hopefully get used to it.\\\n\n### QUESTION 1: How would you represent these 6 distributions mathematically, using $P_0$'$P$, $D$, $|$, and $\\theta$ ?\n\n::: callout-tip\nHint 1: Many of these terms were in the Bayes Rule.\n:::\n\n### Answer:\n\n> -   the likelihood distributions = P (d \\| theta)\n> -   the prior parameter distribution (aka priors) = P (theta)\n> -   the prior predictive distributions = p (X \\| M) (???)\n> -   the posterior parameter distributions (aka posteriors) = P (theta \\| d )\n> -   the posterior-predictive distribution = P ( theta \\| X, M) (????)\n>\n> We also have some other distributions that follow from these. For example, - the distributions of the model expectations (i.e., the predicted means)\n\n### Likelihood distributions.\n\nWe are approaching Bayesian statistics from a likelihood-based perspective. That is, we situate regression models within the greater context of a likelihood function. (There are ways to do non-parametric Bayesian statistics, which don't focus on likelihoods. We won't get into that right now.)\n\nSo far, we have been using the conventional Gaussian likelihood. If we have some variable $y$, we can express it as normally distributed by\n\n$$\n\\operatorname{Normal}(y \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma}} \\exp \\left( \\frac{1}{2} \\left( \\frac{y - \\mu}{\\sigma}\\right)^2\\right),\n$$\n\nwhere $\\mu$ is the mean and $\\sigma$ is the standard deviation. With this likelihood,\n\n-   $\\mu \\in \\mathbb R$\n    -   the mean can be any real number, ranging from $-\\infty$ to $\\infty$\n-   $\\sigma \\in \\mathbb R_{> 0}$\n    -   the standard deviation can take on any real number greater than zero.\n\nIt's also the assumption\n\n-   $y \\in \\mathbb R$\n    -   the focal variable $y$ can be any real number, ranging from $-\\infty$ to $\\infty$.\n\nOne of the ways we wrote our model formula back in the first file was\n\n$$\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i,\n\\end{align}\n$$\n\nand further in the discussion, we updated that equation with the posterior means for our three parameters to\n\n$$\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, 2.92) \\\\\n\\mu_i & = 32.2 + 0.004 \\text{body_mass_g}_i.\n\\end{align}\n$$\n\nBefore we get into this, though, let's back up and consider an intercept-only model of the form\n\n$$\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 ,\n\\end{align}\n$$\n\nwhere there is no predictor variable. Here's how to fit the model with `brm()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bayes\nfit0.b <- brm(\n  data = chinstrap,\n  bill_length_mm ~ 1\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\nLet's look at the model summary.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit0.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    48.85      0.41    48.03    49.65 1.00     3165     2512\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     3.38      0.30     2.85     4.02 1.00     3554     2630\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\nThe intercept parameter $\\beta_0$ is a stand-in for $\\mu$. The $\\sigma$ parameter is just $\\sigma$. Here they are in a plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws <- as_draws_df(fit0.b) \n\ndraws %>% \n  rename(`beta[0]==mu` = b_Intercept) %>% \n  pivot_longer(`beta[0]==mu`:sigma, names_to = \"parameter\") %>% \n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = .95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\nHere are the posterior means for those two parameters.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu <- mean(draws$b_Intercept)\nsigma <- mean(draws$sigma)\n\nmu; sigma\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 48.84791\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.379112\n```\n\n\n:::\n:::\n\n\n\nWe can use `dnorm()` to compute the shape of $\\operatorname{Normal}(48.8, 3.4)$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(y = seq(from = 30, to = 70, by = 0.1)) %>% \n  mutate(density = dnorm(x = y, mean = mu, sd = sigma)) %>% \n  \n  ggplot(aes(x = y, y = density)) +\n  geom_line() +\n  xlab(\"bill_length_mm\")\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\nWe can compare this to the sample distribution of the `bill_length_mm` data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchinstrap %>% \n  ggplot(aes(x = bill_length_mm)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 binwidth = 2.5) +\n  geom_line(data = tibble(bill_length_mm = seq(from = 30, to = 70, by = 0.1)),\n            aes(y = dnorm(x = bill_length_mm, mean = mu, sd = sigma)),\n            color = \"red\")\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nIt's not a great fit, but not horrible either.\n\nNow let's see what this means for our univariable model `fit1.b`. First, let's learn about the `posterior_summary()` function, which we'll use to save a few posterior means.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_summary(fit1.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   Estimate    Est.Error          Q2.5         Q97.5\nb_Intercept    3.213513e+01 3.4928323957  2.537918e+01  3.885693e+01\nb_body_mass_g  4.473184e-03 0.0009288212  2.694532e-03  6.274884e-03\nsigma          2.916785e+00 0.2524862304  2.484028e+00  3.457039e+00\nIntercept      4.883392e+01 0.3437772910  4.817198e+01  4.949538e+01\nlprior        -4.296495e+00 0.0683263164 -4.451671e+00 -4.183096e+00\nlp__          -1.722608e+02 1.1942029129 -1.753620e+02 -1.708897e+02\n```\n\n\n:::\n\n```{.r .cell-code}\nb0    <- posterior_summary(fit1.b)[1, 1]\nb1    <- posterior_summary(fit1.b)[2, 1]\nsigma <- posterior_summary(fit1.b)[3, 1]\n```\n:::\n\n\n\nNow we plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrossing(body_mass_g    = seq(from = 2500, to = 5000, length.out = 200),\n         bill_length_mm = seq(from = 35, to = 60, length.out = 200))  %>% \n  mutate(density = dnorm(x = bill_length_mm, \n                         mean = b0 + b1 * body_mass_g,\n                         sd = sigma)) %>% \n  \n  ggplot(aes(x = body_mass_g, y = bill_length_mm)) +\n  geom_raster(aes(fill = density),\n              interpolate = TRUE) +\n  geom_point(data = chinstrap,\n             shape = 21, color = \"white\", fill = \"black\", stroke = 1/4) +\n  scale_fill_viridis_c(option = \"A\", begin = .15, limits = c(0, NA)) +\n  coord_cartesian(xlim = range(chinstrap$body_mass_g),\n                  ylim = range(chinstrap$bill_length_mm))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\nOur univariable model `fit1.b` can be viewed as something like a 3-dimensional Gaussian hill.\n\n### Prior distributions & Prior predictive distributions.\n\nLet's hold off on this for a bit.\n\n### Parameter distributions.\n\nUp above, we plotted the posterior distributions for our intercept-only `fit0.b` model. Here they are again.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws %>% \n  rename(`beta[0]==mu` = b_Intercept) %>% \n  pivot_longer(`beta[0]==mu`:sigma, names_to = \"parameter\") %>% \n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = .99, normalize = \"panels\",\n               # customize some of the aesthetics\n               fill = \"lightskyblue1\", color = \"royalblue\", \n               point_color = \"darkorchid4\", point_size = 4, shape = 15) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"fit0.b\",\n       subtitle = \"This time we used 99% intervals, and got silly with the colors.\",\n       x = \"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\nWe might practice making a similar plot for our univariable model `fit1.b`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas_draws_df(fit1.b) %>% \n  rename(`beta[0]` = b_Intercept,\n         `beta[1]` = b_body_mass_g) %>% \n  pivot_longer(cols = c(`beta[0]`, `beta[1]`, sigma), \n               names_to = \"parameter\") %>% \n  \n  ggplot(aes(x = value)) +\n  stat_histinterval(.width = .95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"fit1.b\",\n       subtitle = \"Using good old 95% intervals, but switching to histograms\",\n       x = \"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\nSome authors, like John Kruschke, have a strong preference for plotting their posteriors with histograms, rather than density plots.\n\n## Distributions of the model expectations.\n\nTake another look at the `conditional_effects()` plot from earlier.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconditional_effects(fit1.b) %>% \n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\nThe blue line is the posterior mean, for the $\\mu_i$, the model-based mean for `bill_length_mm`, given the value for the predictor `body_mass_g`. The semitransparent gray ribbon marks the percentile-based interval for the conditional mean.\n\nWe can make a similar plot with the `fitted()` function. First we'll need a predictor grid, we'll call `nd`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnd <- tibble(body_mass_g = seq(\n  from = min(chinstrap$body_mass_g),\n  to = max(chinstrap$body_mass_g),\n  length.out = 100))\n\nglimpse(nd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 100\nColumns: 1\n$ body_mass_g <dbl> 2700.000, 2721.212, 2742.424, 2763.636, 2784.848, 2806.061…\n```\n\n\n:::\n:::\n\n\n\nNow pump `nd` into the `fitted()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, newdata = nd) %>% \n  # subset the first 6 rows\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 44.21272 1.0272599 42.21902 46.25100\n[2,] 44.30761 1.0087145 42.34082 46.31054\n[3,] 44.40250 0.9902137 42.47320 46.37193\n[4,] 44.49738 0.9717602 42.60615 46.42913\n[5,] 44.59227 0.9533567 42.73946 46.48624\n[6,] 44.68715 0.9350061 42.88018 46.54543\n```\n\n\n:::\n:::\n\n\n\nNow plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, newdata = nd) %>% \n  data.frame() %>% \n  bind_cols(nd) %>% \n  ggplot(aes(x = body_mass_g)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/3) +\n  geom_line(aes(y = Estimate)) +\n  # add the data\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\nLook what happens if we augment the `probs` argument in `fitted()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, \n       newdata = nd,\n       probs = c(.025, .975, .25, .75)) %>% \n  data.frame() %>% \n  bind_cols(nd) %>% \n  ggplot(aes(x = body_mass_g)) +\n  # 95% range\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # 50% range\n  geom_ribbon(aes(ymin = Q25, ymax = Q75),\n              alpha = 1/4) +\n  geom_line(aes(y = Estimate)) +\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\nNow look what happens if we set `summary = FALSE`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, \n       newdata = nd,\n       summary = FALSE) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n num [1:4000, 1:100] 42.9 45.6 42.7 44.6 42.1 ...\n```\n\n\n:::\n:::\n\n\n\nWe get full 4,000 draw posterior distributions for each of the 100 levels of the predictor `body_mass_g`. Now look at what happens if we wrangle that output a little, and plot with aid from `stat_lineribbon()` from the **ggdist** package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, \n       newdata = nd,\n       summary = F) %>% \n  data.frame() %>% \n  set_names(pull(nd, body_mass_g)) %>% \n  mutate(draw = 1:n()) %>% \n  pivot_longer(-draw) %>% \n  mutate(body_mass_g = as.double(name)) %>%\n  \n  ggplot(aes(x = body_mass_g, y = value)) +\n  stat_lineribbon() +\n  scale_fill_brewer() +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm)) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\nLook what happens when we request more intervals in the `.width` argument.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, \n       newdata = nd,\n       summary = F) %>% \n  data.frame() %>% \n  set_names(pull(nd, body_mass_g)) %>% \n  mutate(draw = 1:n()) %>% \n  pivot_longer(-draw) %>% \n  mutate(body_mass_g = as.double(name)) %>%\n  \n  ggplot(aes(x = body_mass_g, y = value)) +\n  # make more ribbons\n  stat_lineribbon(.width = c(.1, .2, .3, .4, .5, .6, .7, .8, .9),\n                  # remove the line\n                  linewidth = 0) +\n  scale_fill_brewer() +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm)) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\nThe conditional mean, $\\mu_i$, has its own distribution. We can take this visualization approach even further to make a color gradient.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, \n       newdata = nd,\n       summary = F) %>% \n  data.frame() %>% \n  set_names(pull(nd, body_mass_g)) %>% \n  mutate(draw = 1:n()) %>% \n  pivot_longer(-draw) %>% \n  mutate(body_mass_g = as.double(name)) %>%\n  \n  ggplot(aes(x = body_mass_g, y = value, fill = after_stat(.width))) +\n  # make more ribbons\n  stat_lineribbon(.width = ppoints(50)) +\n  scale_fill_distiller(limits = 0:1) +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm)) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\nFor technical details on this visualization approach, go here: <https://mjskay.github.io/ggdist/articles/lineribbon.html#lineribbon-gradients>.\n\nThe **ggdist** package even has an experimental visualization approach that's based on density gradients, rather than interval-width gradients. Since this is experimental, I'm not going to go into the details. But if you're curious and adventurous, you can learn more here: <https://mjskay.github.io/ggdist/articles/lineribbon.html#lineribbon-density-gradients>.\n\n### Posterior-predictive distributions.\n\nThe last section showed the posterior distributions for the model expectations (i.e., the conditional means). In the context of the Gaussian distribution, that's $\\mu$, or $\\mu_i$ in the case of the univariable model `fit1.b`. But the whole Gaussian distribution includes $\\mu$ and $\\sigma$.\n\nThis is where the `predict()` function comes in. First, we compare the `fitted()` output to `predict()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, newdata = nd) %>% \n  # subset the first 6 rows\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 44.21272 1.0272599 42.21902 46.25100\n[2,] 44.30761 1.0087145 42.34082 46.31054\n[3,] 44.40250 0.9902137 42.47320 46.37193\n[4,] 44.49738 0.9717602 42.60615 46.42913\n[5,] 44.59227 0.9533567 42.73946 46.48624\n[6,] 44.68715 0.9350061 42.88018 46.54543\n```\n\n\n:::\n\n```{.r .cell-code}\npredict(fit1.b, newdata = nd) %>% \n  # subset the first 6 rows\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 44.20772  3.102388 38.18415 50.42347\n[2,] 44.21876  3.046605 38.26800 50.33687\n[3,] 44.52772  3.082757 38.42673 50.51053\n[4,] 44.52925  3.044405 38.64203 50.48403\n[5,] 44.71123  3.106361 38.60326 50.69013\n[6,] 44.67384  3.087186 38.51992 50.73298\n```\n\n\n:::\n:::\n\n\n\nThe posterior means (`Estimate`) are about the same, but the SD's (`Est.Error`) are much larger in the `predict()` output, and the widths of the 95% intervals are too. Let's make a plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit1.b, newdata = nd) %>% \n  data.frame() %>% \n  bind_cols(nd) %>% \n  ggplot(aes(x = body_mass_g)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/3) +\n  geom_line(aes(y = Estimate)) +\n  # add the data\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm)) +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\nThe gray band is the 95% interval for the entire posterior predictive distribution, not just the mean. In a good model, about 95% of the data points should be within those bands.\n\nDiscuss how the jagged lines have to do with the uncertainty in $\\sigma$.\n\nIf we wanted to, we could integrate the `fitted()`-based conditional posterior mean, with the `predict()`-based posterior-predictive distribution.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# save the fitted() results\nf <- fitted(fit1.b, newdata = nd) %>% \n  data.frame() %>% \n  bind_cols(nd) \n\npredict(fit1.b, newdata = nd) %>% \n  data.frame() %>% \n  bind_cols(nd) %>% \n  \n  ggplot(aes(x = body_mass_g)) +\n  # 95% posterior-predictive range\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # 95% conditional mean range\n  geom_ribbon(data = f,\n              aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # posterior mean of the conditional mean\n  geom_line(data = f,\n            aes(y = Estimate)) +\n  # original data\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm)) +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n\nIt's the posterior predictive distribution that we use to predict new data points. For example, here's what happens if we use `predict()` without the `newdata` argument.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit1.b) %>% \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 47.80465  2.954352 42.00147 53.71470\n[2,] 49.60112  2.966946 43.97924 55.44956\n[3,] 48.44220  2.943248 42.47971 54.30070\n[4,] 47.89926  2.959338 42.14834 53.63972\n[5,] 48.89363  2.928535 43.23482 54.63086\n[6,] 49.78517  2.927893 44.00176 55.53297\n```\n\n\n:::\n:::\n\n\n\nWe get posterior predictive summaries for each of the original data points. Here's what happens if we set `summary = FALSE`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit1.b, summary = FALSE) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n num [1:4000, 1:68] 50.1 48.7 46.2 51 45.5 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n```\n\n\n:::\n:::\n\n\n\nThis time, we got 4,000 posterior draws for each. We can reduce that output with the `ndraws` argument.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit1.b, summary = FALSE, ndraws = 6) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n num [1:6, 1:68] 48.4 50.7 52.1 44.7 50.1 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n```\n\n\n:::\n:::\n\n\n\nNow wrangle and plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\npredict(fit1.b, summary = FALSE, ndraws = 6) %>% \n  data.frame() %>% \n  mutate(draw = 1:n()) %>% \n  pivot_longer(-draw) %>% \n  mutate(row = str_remove(name, \"X\") %>% as.double()) %>% \n  left_join(chinstrap %>% \n              mutate(row = 1:n()),\n            by = join_by(row)) %>% \n  \n  ggplot(aes(x = body_mass_g, y = value)) + \n  geom_point() +\n  ylab(\"bill_length_mm\") +\n  facet_wrap(~ draw, labeller = label_both)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n\nWith `predict()`, we can use the entire posterior-predictive distribution to simulate new data based on the values of our predictor variable(s). To give you a better sense of what's happening under the hood, here's an `as_draws_df()` based alternative.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\n# walk this code through\nas_draws_df(fit1.b) %>% \n  rename(beta0 = b_Intercept,\n         beta1 = b_body_mass_g) %>% \n  select(.draw, beta0, beta1, sigma) %>% \n  slice_sample(n = 6) %>% \n  expand_grid(chinstrap %>% select(body_mass_g)) %>% \n  mutate(bill_length_mm = rnorm(n = n(),\n                                mean = beta0 + beta1 * body_mass_g,\n                                sd = sigma)) %>% \n  \n  ggplot(aes(x = body_mass_g, y = bill_length_mm)) + \n  geom_point() +\n  facet_wrap(~ .draw, labeller = label_both)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n\nNow take a look at what happens when we plot the densities of several simulated draws.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\nas_draws_df(fit1.b) %>% \n  rename(beta0 = b_Intercept,\n         beta1 = b_body_mass_g) %>% \n  select(.draw, beta0, beta1, sigma) %>% \n  slice_sample(n = 50) %>%  # increase the number of random draws\n  expand_grid(chinstrap %>% select(body_mass_g)) %>% \n  mutate(bill_length_mm = rnorm(n = n(),\n                                mean = beta0 + beta1 * body_mass_g,\n                                sd = sigma)) %>% \n  \n  ggplot(aes(x = bill_length_mm, group = .draw)) + \n  geom_density(size = 1/4, color = alpha(\"black\", 1/2)) +\n  coord_cartesian(xlim = range(chinstrap$bill_length_mm) + c(-2, 2))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\n\nThe similarities and differences among the individual density lines give you a sense of the (un)certainty of the posterior-predictive distribution.\n\n**This may be a good time for you to work on Exercise 1 (see end of the document)**\n\n#Part 4: Beginning to look at priors\n\n## Bayes' rule\n\nBayes' theorem will allow us to determine the plausibility of various values of our parameter(s) of interest, $\\theta$, given the data $d$, which we can express formally as $\\Pr(\\theta \\mid d)$. Bayes' rule takes on the form\n\n$$\n\\Pr(\\theta \\mid d) = \\frac{\\Pr(d \\mid \\theta) \\Pr(\\theta)}{\\Pr(d)}.\n$$\n\nwhere\n\n-   $\\Pr(d \\mid \\theta)$ is the *likelihood*,\n-   $\\Pr(\\theta)$ is the *prior*,\n-   $\\Pr(d)$ is the *average probability of the data*, and\n-   $\\Pr(\\theta \\mid d)$ is the *posterior*.\n\nWe can express this in words as\n\n$$\n\\text{Posterior} = \\frac{\\text{Probability of the data} \\times \\text{Prior}}{\\text{Average probability of the data}}.\n$$\n\nThe denominator $\\Pr(d)$ is a normalizing constant, and dividing by this constant is what converts the posterior $\\Pr(\\theta \\mid d)$ into a probability metric.\n\n## Default priors\n\nTo set your priors with **brms**, the `brm()` function has a `prior` argument. If you don't explicitly use the `prior` argument, `brm()` will use default priors. This is what happened with our `fit1.b` model from above. We used default priors. If you'd like to see what those priors are, execute `fit1.b$prior`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# maybe show str(fit1.b)\nfit1.b$prior\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   prior     class        coef group resp dpar nlpar lb ub\n                  (flat)         b                                        \n                  (flat)         b body_mass_g                            \n student_t(3, 49.5, 3.6) Intercept                                        \n    student_t(3, 0, 3.6)     sigma                                    0   \n       source\n      default\n (vectorized)\n      default\n      default\n```\n\n\n:::\n:::\n\n\n\nThus, a fuller expression of our model is\n\n$$\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i \\\\\n\\beta_0 & \\sim \\operatorname{Student-t}(3, 49.5, 3.6) \\\\\n\\beta_1 & \\sim \\operatorname{Uniform}(-\\infty, \\infty) \\\\\n\\sigma & \\sim \\operatorname{Student-t}^+(3, 0, 3.6).\n\\end{align}\n$$\n\nIf we had wanted to see the `brm()` defaults before fitting the model, we could have used the `get_prior()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_prior(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   prior     class        coef group resp dpar nlpar lb ub\n                  (flat)         b                                        \n                  (flat)         b body_mass_g                            \n student_t(3, 49.5, 3.6) Intercept                                        \n    student_t(3, 0, 3.6)     sigma                                    0   \n       source\n      default\n (vectorized)\n      default\n      default\n```\n\n\n:::\n:::\n\n\n\nIf you recall, the normal distribution is a member of the Student-t family, where the $\\nu$ (aka degrees of freedom or normality parameter) is set to $\\infty$. To give you a sense, here are the densities of three members of the Student-t family, with varying $\\nu$ values.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrossing(theta = seq(from = -4.5, to = 4.5, length.out = 200),\n         nu = c(3, 10, Inf)) %>% \n  mutate(density = dt(x = theta, df = nu)) %>% \n  \n  ggplot(aes(x = theta, y = density, color = factor(nu))) +\n  geom_line(linewidth = 1) +\n  scale_color_viridis_d(expression(nu), option = \"A\", end = .7) +\n  labs(title = \"3 members of the Student-t family\",\n       x = expression(theta)) +\n  coord_cartesian(xlim = c(-4, 4))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n\nThus, Student-t distributions have thicker tails when they have smaller $\\nu$ parameters. In the case where $\\nu = 3$, the tails are pretty thick, which means they are more tolerant of more extreme values. And thus priors with small-$\\nu$ parameters will be weaker (i.e., more permissive) than their Gaussian counterparts.\n\nWe can visualize functions from **ggdist** to visualize the default `brm()` priors. We'll start with the `student_t(3, 49.5, 3.6)` $\\beta_0$ prior, and also take the opportunity to compare that with a slightly stronger `normal(49.5, 3.6)` alternative.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(prior(student_t(3, 49.5, 3.6)),\n  prior(normal(49.5, 3.6))) %>% \n  parse_dist() %>% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye() +\n  labs(x = expression(italic(p)(beta[0])),\n       y = NULL) +\n  coord_cartesian(xlim = c(25, 75))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n\nSee how that $n = 3$ parameter in the default prior let do much thicker tails than it's Gaussian counterpart. We can make the same kind of plot for our default $\\sigma$ prior and its half-Gaussian counterpart.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(prior(student_t(3, 0, 3.6), lb = 0),  # note our use of the lb = 0 argument\n  prior(normal(0, 3.6), lb = 0)) %>% \n  parse_dist() %>% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(point_interval = mean_qi, .width = c(.90, .99)) +\n  labs(x = expression(italic(p)(sigma)),\n       y = NULL) +\n  coord_cartesian(xlim = c(0, 30))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\n\nHere's how we could have explicitly set our priors by hand.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit2.b <- brm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g,\n  prior = prior(student_t(3, 49.5, 3.6), class = Intercept) +\n    prior(student_t(3, 0, 3.6), class = sigma, lb = 0)\n)\n```\n:::\n\n\n\nCompare the results.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit1.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      32.14      3.49    25.38    38.86 1.00     5182     3372\nbody_mass_g     0.00      0.00     0.00     0.01 1.00     5251     3165\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.92      0.25     2.48     3.46 1.00     2086     1650\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(fit2.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      32.13      3.57    24.99    38.99 1.00     4691     2986\nbody_mass_g     0.00      0.00     0.00     0.01 1.00     4740     2970\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.94      0.27     2.47     3.52 1.00     2019     1693\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\n## QUESTION 2 Are the priors the same? What do you think is going on?\n\n### Answer:\n\n> The differences between the priors are quite minimal and probably it's just natural variation from sampling?\n>\n> If you want to learn more about the default prior settings for **brms**, read through the `set_prior` section of the **brms** reference manual (https://CRAN.R-project.org/package=brms/brms.pdf).\n\n# EXERCISE 1\n\nIn the previous lab, we made a subset of the `penguins` data called `gentoo`, which was only the cases for which `species == \"Gentoo\"`. Do that again and refit the Bayesian model to those data. Remake some of the figures (From Part 3) in this file with the new version of the model?\n\n### Answer/ Your solution below: ....\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngentoo <- penguins %>%\n  subset(species == \"Gentoo\")\n\nmodel <- brm(\n  data = gentoo,\n  bill_length_mm ~ 1 + body_mass_g\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Rows containing NAs were excluded from the model.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.8e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.036 seconds (Warm-up)\nChain 1:                0.022 seconds (Sampling)\nChain 1:                0.058 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.029 seconds (Warm-up)\nChain 2:                0.023 seconds (Sampling)\nChain 2:                0.052 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.027 seconds (Warm-up)\nChain 3:                0.023 seconds (Sampling)\nChain 3:                0.05 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.034 seconds (Warm-up)\nChain 4:                0.022 seconds (Sampling)\nChain 4:                0.056 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: gentoo (Number of observations: 123) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      26.76      2.16    22.45    31.04 1.00     4339     2898\nbody_mass_g     0.00      0.00     0.00     0.00 1.00     4357     2864\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.32      0.15     2.05     2.63 1.00     1939     1683\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws <- as_draws_df(model) \n\ndraws %>% \n  rename(`beta[0]==mu` = b_Intercept) %>% \n  pivot_longer(`beta[0]==mu`:sigma, names_to = \"parameter\") %>% \n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = .95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmu <- mean(draws$b_Intercept)\nsigma <- mean(draws$sigma)\n\nmu; sigma\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 26.75887\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.317695\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(y = seq(from = 30, to = 70, by = 0.1)) %>% \n  mutate(density = dnorm(x = y, mean = mu, sd = sigma)) %>% \n  \n  ggplot(aes(x = y, y = density)) +\n  geom_line() +\n  xlab(\"bill_length_mm\")\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nchinstrap %>% \n  ggplot(aes(x = bill_length_mm)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 binwidth = 2.5) +\n  geom_line(data = tibble(bill_length_mm = seq(from = 30, to = 70, by = 0.1)),\n            aes(y = dnorm(x = bill_length_mm, mean = mu, sd = sigma)),\n            color = \"red\")\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_summary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   Estimate   Est.Error          Q2.5         Q97.5\nb_Intercept    2.675887e+01 2.155021232  2.244962e+01  3.103834e+01\nb_body_mass_g  4.087362e-03 0.000422499  3.241679e-03  4.939186e-03\nsigma          2.317695e+00 0.147674771  2.053994e+00  2.629821e+00\nIntercept      4.750639e+01 0.210317936  4.708526e+01  4.792033e+01\nlprior        -3.920021e+00 0.041227101 -4.009981e+00 -3.848205e+00\nlp__          -2.805505e+02 1.267143636 -2.837201e+02 -2.791407e+02\n```\n\n\n:::\n\n```{.r .cell-code}\nb0    <- posterior_summary(model)[1, 1]\nb1    <- posterior_summary(model)[2, 1]\nsigma <- posterior_summary(model)[3, 1]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncrossing(body_mass_g    = seq(from = 2500, to = 5000, length.out = 200),\n         bill_length_mm = seq(from = 35, to = 60, length.out = 200))  %>% \n  mutate(density = dnorm(x = bill_length_mm, \n                         mean = b0 + b1 * body_mass_g,\n                         sd = sigma)) %>% \n  \n  ggplot(aes(x = body_mass_g, y = bill_length_mm)) +\n  geom_raster(aes(fill = density),\n              interpolate = TRUE) +\n  geom_point(data = chinstrap,\n             shape = 21, color = \"white\", fill = \"black\", stroke = 1/4) +\n  scale_fill_viridis_c(option = \"A\", begin = .15, limits = c(0, NA)) +\n  coord_cartesian(xlim = range(chinstrap$body_mass_g),\n                  ylim = range(chinstrap$bill_length_mm))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-43-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws %>% \n  rename(`beta[0]==mu` = b_Intercept) %>% \n  pivot_longer(`beta[0]==mu`:sigma, names_to = \"parameter\") %>% \n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = .99, normalize = \"panels\",\n               # customize some of the aesthetics\n               fill = \"lightskyblue1\", color = \"royalblue\", \n               point_color = \"darkorchid4\", point_size = 4, shape = 15) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"fit0.b\",\n       subtitle = \"This time we used 99% intervals, and got silly with the colors.\",\n       x = \"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-44-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nas_draws_df(model) %>% \n  rename(`beta[0]` = b_Intercept,\n         `beta[1]` = b_body_mass_g) %>% \n  pivot_longer(cols = c(`beta[0]`, `beta[1]`, sigma), \n               names_to = \"parameter\") %>% \n  \n  ggplot(aes(x = value)) +\n  stat_histinterval(.width = .95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"model\",\n       subtitle = \"Using good old 95% intervals, but switching to histograms\",\n       x = \"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-45-1.png){width=672}\n:::\n:::\n\n\n\n> I think I've learned that I like the density plots more than the histograms, I find it to be easier to visualize!\n\n## References\n\nKruschke, J. K. (2015). *Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan*. Academic Press. <https://sites.google.com/site/doingbayesiandataanalysis/>\n\n## Session information\n",
    "supporting": [
      "Bayes_Lab_2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}