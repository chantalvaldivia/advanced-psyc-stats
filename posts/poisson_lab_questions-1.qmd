---
title: "Lab 5 - Poisson - Questions"
author:
    date: last-modified
    format:
      html:
        self-contained: true
        anchor-sections: true
        code-tools: true
        code-fold: true
        fig-width: 8
        fig-height: 4
        code-block-bg: "#f1f3f5"
        code-block-border-left: "#31BAE9"
        mainfont: Source Sans Pro
        theme: journal
        toc: true
        toc-depth: 3
        toc-location: left
        captions: true
        cap-location: margin
        table-captions: true
        tbl-cap-location: margin
        reference-location: margin
      pdf:
        pdf-engine: lualatex
        toc: false
        number-sections: true
        number-depth: 2
        top-level-division: section
        reference-location: document
        listings: false
        header-includes:
          \usepackage{marginnote, here, relsize, needspace, setspace}
          \def\it{\emph}
execute:
  freeze: auto
  echo: true
  message: false
  warning: false
  fig-align: center
  fig-width: 12
  fig-height: 8
  editor_options: 
  chunk_output_type: inline
  code-overflow: wrap
  html:
    code-fold: true
    code-tools: true
---

1.  To complete this lab:

-   Load packages

```{r}
library(MASS)
library(tidyverse)
library(emmeans)
library(ggeffects)
library(easystats)
library(performance)
library(knitr)
```

-   Download the dataset:

```{r}

library(tidyverse)

data <- read_delim("https://raw.githubusercontent.com/jgeller112/psy504-advanced-stats/main/slides/Poisson/data/2010.csv")

```

2.  Conduct the analysis described in the preregistration document

<!-- -->

a.  The number of hours per week that a person spends on the Internet ("WWWHR") will\
    be predicted by their vocabulary ("WORDSUM"), age ("AGE"), sex ("SEX"), religiosity\
    ("RELITEN"), political orientation ("POLVIEWS"), and how often they work from home\
    ("WRKHOME").

-   Let's use the `naniar` package's function `replace_with_na`to clean the data.

```{r}
library(naniar)

data_pos <- data %>%
  dplyr::select(wwwhr, wordsum, age, sex, reliten, polviews, wrkhome) %>%
replace_with_na(.,
             replace = list(wwwhr = c(-1, 998, 999),
                          wordsum = c(-1, 99),
                          reliten = c(0, 8, 9), 
             polviews = c(0, 8, 9), 
             wrkhome = c(0,8,9), 
             age=c(0, 98, 99)))
```

Q: Can you explain what might be going on in the above code?

**A: It seems like we are using dplyr to replace some codes (like 0, 8, 9, -1, 99, etc) with NAs.**

Q: The next step in data cleaning would be to ensure that the data in your code are aligned with the description/ usage context of the variables

-   Recode sex and reliten as necessary

```{r}
data_pos <- data_pos %>% 
  mutate(sex = recode(sex, `-1` = "Male", `1` = "Female"))

data_pos$sex <- as.factor(data_pos$sex)

data_pos <- data_pos %>% 
    mutate(reliten_recode = factor(reliten, levels = c(1, 2, 3, 4, 5), ordered = TRUE))

```

## Missingness

```{r}
data_pos %>%
  dplyr::select(reliten, reliten_recode)


library(skimr)
skimr::skim(data_pos)

```

## Fit a Poisson model to the data.

a.  The number of hours per week that a person spends on the Internet ("WWWHR") will\
    be predicted by their vocabulary ("WORDSUM"), age ("AGE"), sex ("SEX"), religiosity\
    ("RELITEN"), political orientation ("POLVIEWS"), and how often they work from home\
    ("WRKHOME").

```{r}

model <- glm(wwwhr ~ wordsum + age + sex + reliten_recode + polviews + wrkhome,                  data = data_pos, 
                 family = poisson(link = "log"))

library(broom)
model %>%
  tidy()
```

## Carry out model checking

Hint: performance package has the function you're looking for

```{r}
check_model(model,plot = T)
```

## Find any outliers

```{r}
#Q <- quantile(data_pos$wwwhr, probs=c(.25, .75), na.rm = T)
#iqr <- IQR(data_pos$wwwhr, na.rm = T)

#up <-  Q[2]+1.5*iqr # Upper Range  
#low<- Q[1]-1.5*iqr # Lower Range

#subset(data_pos, data_pos$wwwhr >= (Q[1] - 1.5*iqr) & data_pos$wwwhr <= (Q[2]+1.5*iqr))

# I tried this method above and it cut almost 1000 observations. Any thoughts to why this happened? I'm confused...

stats <- data_pos %>%
  summarize(mean_wwwhr = mean(wwwhr, na.rm = TRUE),
            sd_wwwhr = sd(wwwhr, na.rm = TRUE))

data_pos_filtered <- data_pos %>%
  filter(wwwhr >= (stats$mean_wwwhr - 3 * stats$sd_wwwhr) & 
         wwwhr <= (stats$mean_wwwhr + 3 * stats$sd_wwwhr))



check_outliers(model)
```

## Refit the model after excluding outliers

```{r}
  
  new_model <- glm(wwwhr ~ wordsum + age + sex + reliten_recode + polviews + wrkhome,                  data = data_pos_filtered, 
                 family = poisson(link = "log"))
```

```{r}
model_parameters(new_model) %>%
  print_html()
```

### Check for Overdispersion

Hint: performance package has the function you're looking for

```{r}
check_model(new_model,plot = T)
```

What do you notice? And what's a good next step forward? Can there be another model class that can fit the data? If so, fit this model to the data.

I notice that we no longer have an outlier problem, but we do have an overdispersion problem. The next step would proably use a negative binomial regression model.

```{r}
check_overdispersion(new_model)
```

```{r}
  binom_model <- glm.nb(wwwhr ~ wordsum + age + sex + reliten_recode + polviews + wrkhome, data = data_pos_filtered)

summary(binom_model)
model_parameters(binom_model) %>%
  print_html()
```

## Which one is better- your earlier model, or later model?

The negative binomial model (so the later model) has a much lower AIC, so we can conclude the model fits the data better than the poisson

```{r}
test_likelihoodratio(new_model, binom_model)
AIC(new_model, binom_model)
```

## What is zero inflation? Is there zero-inflation in your chosen model?

When there are too many zeros in your data, that can bias your result. There is indeed zero inflation in my chosen model.

```{r}
performance::check_zeroinflation(binom_model)

```

::: panel-tabset
## Log Lambda

```{r}
lambda_poisson <- predict(new_model, type = "response")
lambda_negbinom <- predict(binom_model, type = "response")

# Log transform the lambda values
log_lambda_poisson <- log(lambda_poisson)
log_lambda_negbinom <- log(lambda_negbinom)

# Output the log lambda values for a subset of the data
head(data.frame(log_lambda_poisson, log_lambda_negbinom))

```

## Mean Count

```{r}
predicted_counts_poisson <- predict(new_model, type = "response")
predicted_counts_negbinom <- predict(binom_model, type = "response")

# Calculate the mean of predicted counts for each model
mean_count_poisson <- mean(predicted_counts_poisson, na.rm = TRUE)
mean_count_negbinom <- mean(predicted_counts_negbinom, na.rm = TRUE)

# Output the mean counts
mean_count_poisson
mean_count_negbinom

```
:::

## Report your conclusions

It seems like the negative binomial model outputs slightly higher mean counts and log lambdas than the poisson distribution. It seems like we should favor the negative binomial outputs over the poisson.

We initally fitted a poisson model to predict the number of hours per week that a person spends on the Internet ("WWWHR") using their vocabulary ("WORDSUM"), age ("AGE"), sex ("SEX"), religiosity ("RELITEN"), political orientation ("POLVIEWS"), and how often they work from home ("WRKHOME") as fixed effects. We then fit a negative binomial model instead because there were too many outliers, in this model, only the effects of wordsum, age, and religiosity (linear) were significant but not those of sex, polviews, or working from home.
